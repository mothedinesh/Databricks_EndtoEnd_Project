{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d67967cb-11e1-4c9d-a006-dab5a6c52f64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### DLT Notes\n",
    "\n",
    "We cannot make changes directly in dlt pipeline since we cannot see the data. \n",
    "We can only see the data once the pipeline is in development mode. This may cause some issues.\n",
    "\n",
    "Instead: Read the dataframe, Make required Transformations and convert the code into DLT\n",
    "\n",
    "Note: We cannot run a DLT pipeline with in the notebook. The Delta Live Tables (DLT) module is not supported on this cluster. You should either create a new pipeline or use an existing pipeline to run DLT code.\n",
    "\n",
    "We can Create three things in DLT:\n",
    "- Streaming table\n",
    "- Materialized view\n",
    "- Views -Normal or Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa3fcfc9-7c12-4508-9584-0b5e3b63ceed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a0b63d2-fff4-4db3-8814-23a8efa536b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read the dataframe\n",
    "\n",
    "df = spark.read.format(\"delta\")\\\n",
    "          .load(\"/Volumes/workspace/bronze/bronzevolume/bookings/data\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c710a4c9-ee75-466a-b508-eb5b9565981f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Make required Transformations\n",
    "\n",
    "df = df.withColumn(\"amount\",col(\"amount\").cast(DoubleType()))\\\n",
    "        .withColumn(\"ModifiedDate\",current_timestamp())\\\n",
    "        .withColumn(\"booking_date\",to_date(col(\"booking_date\")))\\\n",
    "        .drop(\"_rescued_data\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6646fd6-72a3-46b8-a6f5-4424d90ccc1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Convert the code into DLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34db6c3a-7f7d-40a8-8508-56abb823d668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load Data Incrementally\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"stage_bookings\"\n",
    ")\n",
    "def stage_bookings():\n",
    "    return df\n",
    "\n",
    "    df = spark.readStream.format(\"delta\")\\\n",
    "            .load(\"/Volumes/workspace/bronze/bronzevolume/bookings/data\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f1bbfe-3531-4734-9342-cb8de3becb10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create Streaming View\n",
    "\n",
    "@dlt.view(\n",
    "  name = \"trans_bookings\"\n",
    ")\n",
    "def trans_bookings():\n",
    "\n",
    "    df = spark.readStream.table(\"stage_bookings\")\n",
    "    df = df.withColumn(\"amount\",col(\"amount\").cast(DoubleType()))\\\n",
    "            .withColumn(\"ModifiedDate\",current_timestamp())\\\n",
    "            .withColumn(\"booking_date\",to_date(col(\"booking_date\")))\\\n",
    "            .drop(\"_rescued_data\")\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "581d12b5-5daf-47c5-ab96-a3c0a480462b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create Dictionary for rules\n",
    "\n",
    "rules = {\n",
    "    \"rule1\" : \"booking_id IS NOT NULL\",\n",
    "    \"rule2\" : \"passenger_id IS NOT NULL\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b951a84-5680-403b-9f1e-73b117299ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Write data from Streaming view into a Streaming Table\n",
    "\n",
    "@dlt.table(\n",
    "    name = \"silver_bookings\"\n",
    ")\n",
    "#@dlt.expect_all(rules) #This will check if above rules are meeting. If not it will throw warning\n",
    "@dlt.expect_all_or_drop(rules) #This will check if above rules are meeting. If not it drop records\n",
    "#@dlt.expect_all_or_fail(rules) #This will check if above rules are meeting. If not it will fail\n",
    "def silver_bookings():\n",
    "\n",
    "    df = spark.read_stream(\"trans_bookings\")\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SilverLayer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
